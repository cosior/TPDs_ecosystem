{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreation of Figure 2 \n",
    "## Hyperbolic map of the legal-entities network \n",
    "Step 1: Load the nodes coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of nodes: 1215\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# The Legal-entities network with the nodes coordinates \n",
    "f_name = \"../Datasets/EdgeList_Legal_Entities.inf_coord\"\n",
    "\n",
    "header_map = {\n",
    "    \"Vertex\": \"id\",\n",
    "    \"Inf.Kappa\": \"kappa\",\n",
    "    \"Inf.Theta\": \"theta_rad\",\n",
    "    \"Inf.Hyp.Rad.\": \"r\"\n",
    "}\n",
    "\n",
    "# The final dataset\n",
    "dataset = {}\n",
    "# The output header variables\n",
    "header = []\n",
    "# The network nodes \n",
    "nodes = []\n",
    "# The embedding parameters\n",
    "parameters = {}\n",
    "\n",
    "# Read the Mercator output file line by line\n",
    "with open(f_name, \"r\") as fp:\n",
    "    for l in fp.readlines():\n",
    "        # Line cleanup step\n",
    "        line = re.sub(' +', ' ', l.strip())\n",
    "        # Process Comment line\n",
    "        if line.startswith(\"#\"):\n",
    "            # Extract parameters\n",
    "            if \" - \" in line:\n",
    "                tmp = line.replace(\"# - \", \"\").split(\": \")\n",
    "                parameters[tmp[0]] = float(tmp[1])\n",
    "            # Extract data header\n",
    "            elif line.startswith(\"# Vertex\"):\n",
    "                tmp = line.replace(\"# \", \"\").split(\" \")\n",
    "                for e in tmp:\n",
    "                    header.append(header_map[e])\n",
    "        else: # Process nodes data\n",
    "            tmp = line.split(\" \")\n",
    "            t = {}\n",
    "            for k, v in zip(header, tmp):\n",
    "                t[k] = v\n",
    "            nodes.append(t)\n",
    "\n",
    "dataset[\"parameters\"] = parameters\n",
    "dataset[\"nodes\"] = nodes\n",
    "\n",
    "print(\"# of nodes: {}\".format(len(dataset[\"nodes\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load the links between the nodes in the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of links: 5920\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "links = []\n",
    "\n",
    "# Load the Legal-entities edge list \n",
    "with open(\"../Datasets/EdgeList_Legal_Entities.txt\", \"r\") as fp:\n",
    "    for l in fp.readlines():\n",
    "        line = re.sub(' +', ' ',l).strip().split(\" \")\n",
    "        links.append({\n",
    "            \"source\": line[0],\n",
    "            \"target\": line[1]\n",
    "        })\n",
    "dataset[\"links\"] = links\n",
    "print(\"# of links: {}\".format(len(dataset[\"links\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Calculate nodes degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sorted nodes by degree: 1215\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import operator\n",
    "\n",
    "from_n = []\n",
    "to_n = []\n",
    "\n",
    "for l in dataset[\"links\"]:\n",
    "    from_n.append(l[\"source\"])\n",
    "    to_n.append(l[\"target\"])\n",
    "\n",
    "# Build a dataframe with all connections\n",
    "df = pd.DataFrame({'from':from_n, 'to':to_n})\n",
    " \n",
    "# Build the graph\n",
    "G=nx.from_pandas_edgelist(df, 'from', 'to')\n",
    "\n",
    "degrees = dict(G.degree())\n",
    "sorted_degrees = sorted(degrees.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"# of sorted nodes by degree: {}\".format(len(sorted_degrees)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Import required metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country_pal: 56\n",
      "category_pal: 25\n",
      "LocAndType: 1647\n"
     ]
    }
   ],
   "source": [
    "# Import node type colors (pallets)\n",
    "from metadata.pallets import *\n",
    "dataset[\"country_pal\"] = country_pal\n",
    "dataset[\"category_pal\"] = category_pal\n",
    "print(\"country_pal: {}\\ncategory_pal: {}\".format(len(country_pal),len(category_pal)))\n",
    "\n",
    "# Import nodes location and type\n",
    "from metadata.Location_Type import *\n",
    "print(\"LocAndType:\", len(LocAndType))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Annotate network nodes with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of annotated nodes: 1215\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for n in dataset[\"nodes\"]:\n",
    "    n[\"name\"] = n[\"id\"]\n",
    "    n[\"r\"] = float(n[\"r\"])\n",
    "    n[\"theta\"] = float(n[\"theta_rad\"]) * (180 / math.pi)\n",
    "    n[\"degree\"] = degrees[n[\"id\"]]\n",
    "    try:\n",
    "        n[\"Category\"] = LocAndType[n[\"id\"]][\"category\"]\n",
    "        n[\"Country\"] = LocAndType[n[\"id\"]][\"Country\"]\n",
    "    except:\n",
    "        print(n[\"id\"])\n",
    "    \n",
    "print(\"# of annotated nodes: {}\".format(len(dataset[\"nodes\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Save the annotated nodes to a json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f_name + \".json\", \"w\") as fp:\n",
    "    json.dump(dataset, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5ceffe36725d6594f0b4b76fdd457e5b51e88c3fca6822fb7de15c7ce3cac26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
